X comment out DVM logging except for errors

X update DB logging module to remove V_SP_RET_CODE/V_RETURN_CODE
_ update TRG_ history tracking triggers to specify the app username instead of using the dsc_utilities_pkg.os_user function
	use: nvl(v('APP_USER'),user);
_ look into implementing validation criteria into a QA module as well (could have flags to include in QA only or QC only or both)
	_ this could be another module that could query the validation criteria and evaluate it via Ajax or other means to identify errors/warnings for each field and the pending data as a whole.
	In an Ajax call we could pass the values in the given form to the server page
		We could attempt to insert/update the record within a transaction and then run the DVM on the new/updated record to see if it has any error validation criteria and pass back a JSON object with the information
			Rollback the transaction each time but don't throw an error in PL/SQL
			Capture the return value and display errors on the page if there are no errors allow the page to submit normally to process the record update and run the DVM


X implement a unique key constraint for DVM QC object records on OBJECT_NAME (had a use case where I accidentally inserted a duplicate DVM_QC_OBJECTS record and it caused a problem with the generated load DVM issues DML


X compile .md documentation to document the process and scripts used to upgrade the SPTT version of the DVM to the DVM standalone
	X (SPTT - DVM upgrade) compile a mapping document to document the object/column name changes so SPTT can update their code accordingly

X Commit changes to DB v 1.4
	Send them the upgrade scripts and have them try it out themselves
		SPTT needs to implement a repeatable process to deploy the current production version of the DB

_ should we include the external modules in the upgrades folder even if it's not called directly?  That way they don't need to create multiple working copies in order to run all of the necessary scripts






Best Practice:
	_ (document this) Optimize the QC queries as much as possible since they are executed each time a record is validated

	_ document approach for resolving validation issues that may be associated with external data schemas (wrapper table implementation)
		Don't create a special table for this but try to leverage an existing table in the data model unless there is no other option (this creates additional work to create and maintain these records)
		Cite the example (MOUSS on how this was implemented for MOUSS_CR_DATA_LOC for QC validation issues caused by CCD_CRUISES and associated tables)



** DVM QC view: 
	Find all DVM_ISS_TYPES with no corresponding data stream code
	_ add this to the standard set of QC queries to see if the definitions are correct?
	


_ implement a delete DVM procedure that removes the DVM data for a given PTA_ISS_ID value (used in CCD and LIB)

Evaluate DVM for security vulnerabilities (SCR to CMB)


_ ** check on triggers with dates in them (bug -> need to use TO_CHAR() to translate date to string) -> TRG_DVM_PTA_RULE_SETS_HIST

_ document workaround for cases where bracket characters are needed in the URL (APEX interactive grid pages that specify filters that require the region's static ID) -> example in Standalone Authorization Application Module (SAM)


known issues:
	_ DVM will replace the DVM issues records if there is any difference in the description field (e.g. when samp_ID changes it will wipe out any annotated issues)


Documentation:
	X (this is maintained in the CCD repository) compile a quick SOP on how to verify that the data checks are working and how to develop repeatable test cases. (e.g. develop query, create conditions with file names/SQL updates, export the errors and reconcile with test cases to confirm)
	X (added to Google Keep - 7/9/20) document how the batch validation procedure fits into validating production data so it can be used for reporting, data analysis, dissemination, etc.
	X - instructions and scripts for how to configure the given parent table for DVM

	X PL/SQL naming conventions
	X DB Naming conventions
	X How to define validation rules/enable DVM
	X database documentation

	X **business rules

	X Lessons Learned documentation (https://docs.google.com/document/d/1CQEVxOvbmONjPRTirU-8hgellUqv5dwMjLF12N8mGC0/edit?usp=sharing) from 7/2020 development effort to fulfill a PIFSC milestone.  _ We should revisit this when we can and integrate it into the DVM repository

	_ replace DVM_PKG.COMMA_DELIM_LIST_FN with CEN_UTILS function or just replace with the APEX_UTIL.table_to_string()

	X document the history tracking package

functionality:
(1)	X history trigger on DVM_PTA_RULE_SETS
(2)		X work on reports (DVM rules, Cruise DVM Rules, Cruise DVM executions - requires history trigger)

(3)	X QC views to identify problems with the DVM data (use one large union query - YES)
		X data stream, parent record, missing QC view template fields, one rule set is active per data stream, at least one active validation rule for each data stream
			X - application tables: QC query to determine if there are error templates with placeholders that are not included in the result set of the corresponding QC query (DVM_QC_MSG_MISS_FIELDS_V)
		X (added to Google Keep - 7/9/20) error messages or some type of initialization process that indicates (X) which views/objects don't exist (e.g. QC queries) and when (X) references don't exist in the corresponding QC object, etc. so it is clear what the problem is and it can be fixed across the board
			X initialization checks otherwise it won't run properly
	**_ (added to Google Keep - 7/9/20) qc queries for orphaned DVM parent issue records (DVM view)
		_ qc query rule set data stream does not match one of the associated error types' data stream


X implement exceptions instead of return codes for the DVM (this way we can bubble up the errors)  Right now the return code for the VALIDATE_PARENT_RECORD_SP is the only way to tell if it successfully executes.
	X implemented VALIDATE_PARENT_RECORD_RC_SP to provide the status codes for additional use cases
	X document this new functionality



X Implement configuration QC/DVM_PKG check for parent table primary key column does not exist in the data QC view
X Implement configuration QC/DVM_PKG check to see if the indicator fields are not character string fields (need to be in order to be implemented correctly)


_ include a report that shows the DVM_STD_QC_ALL_RPT_V results so admins can view the DVM configuration problems and resolve them for tier-1 troubleshooting (tier-2 would require reviewing DB error logs, etc.)


		_ optimization:
			_ when there is a batch execution order by the validation rule sets so that once a data stream/validation rule set is initialize (validated that the rules are valid at the RULE_SET_ID level then we can skip validating the validation criteria on any subsequent records that have the same rule set - save the values and compare them early on so that a lot of the validation can be skipped - e.g. missing placeholder fields, data stream codes, etc. as long as the validation criteria was valid on the last run)
		
		
		OPTIMIZATION UPDATE:
			_when the batch execution method is used allow the queries to be run for all records at the same time (it will need to take into account which records have which criteria associated with them but this approach should increase efficiency by reducing unnecessary queries)
				Algorithm: Sort by the DVM rules associated with each parent record so that it will reuse the information for each subsequent record and each time the rule set changes then update the rule set in the package variables
				** _ Alternatively it could just retain the package variables for the current rule set and if it doesn't change then there is no need to requery it, just use what's already stored in memory
					The package variables should be reset each time the package is reinitialized

	_ Change views to use consistent date formats (updated the history trigger for trg_DVM_PTA_RULE_SETS_hist so far

	_ Allow a given cruise to be evaluated with all of the active validation criteria (old record that we want to execute new checks on)

	_ **issue types and validation rule sets both define the data stream, is this necessary?

	_ ** - (added to Google Keep - 7/9/20) deprecated flag in the DVM_ISS_TYPES table to indicate that even if an error was active at a certain time it will no longer be evaluated (for invalid/deprecated validation criteria)
		_ Should the view also filter out any occurrences of the deprecated errors - YES (this makes sense, if it's actually deprecated it should be hidden from all reports except for DVM_CRITERIA_V and some others where it would make sense but not DVM_PTA_ISSUES_V and other dependent views)
			RULE SETS should have it as visible
			validation criteria (DVM_ISS_TYPES) should not have it visible
			DVM_ISSUES should be filtered
			PTA Rule Sets ??? (not sure yet - do not include them since they are not active)
			The deprecated issue types should not be included in the history reports? (not sure yet)
			The deprecated issue types should be filtered out of all of the DVM configuration QC queries including the DVM_QC_MSG_MISS_FIELDS_V view
		_ **maybe need to be able to pull the deactivation time from the history table
		_ ** need to include the new field in the history trigger


	_ (added to Google Keep - 7/9/20) not null constraint on error severity

	_ maybe use batch processing to ensure that error records are not deleted and replaced, so anything already there will remain or be disabled.  (This is due to the issues reported by SPTT)

	_ Check that the DB Logging Module is installed before using DVM?

	X implement better error handling for the DVM, so it can fail gracefully and the info can be used to resolve issues

	_ allow records to be re-evaluated with the old rules or new rules

	X optimization updates (queries and PL/SQL code execution)
		_ make the flexible procedures multi-purpose so they can be used by multiple code branches (e.g. RETRIEVE_ACTIVE_RULE_SET_SP)

	X - implement transactions

	X - change DVM_ERRORS.ERROR_DESCRIPTION and DVM_ERROR_TYPES.ERR_TYPE_COMMENT_TEMPLATE to a CLOB data type:

	X Have the DVM_ISS_TYP_ASSOC table reference the rule set record
		X Each time a record is evaluated have the PL/SQL code check to see if the active issue types are the same as the last execution:

	X - need to handle replacing existing errors so that the error records are not purged each time the module is executed:
	    X - The pending and existing error records are compared to determine if the ERROR_DESCRIPTION and ERROR_TYPE_ID are the same.  All existing error records that do not match a pending record are deleted from the database, all matching records are left untouched, and all unmatched pending records are inserted into the database.
	    _ **What if the error description template changes, then it will never match up - how is this handled?

	X - how do we reduce the amount of error type assoc intersection records?  - Use a PTA strategy to define the error types for a given date range:
		X Use a procedure to save the issue records when they're updated (each time they are updated update the currently active PTA issue type assoc record to set the end date to the active date)

	_ Generalize the DVM issues functionality in an APEX application that could be copied and used throughout the Center (potentially a variation of the template application)

	X link to web pages with the corresponding IDs
		for each QC view it can refer to a given page number, and page parameters so it has the information APEX needs to generate the URL
		maybe this should be defined in the DVM_QC_OBJECTS records? -> yes (define APEX page ID, APEX page parameters, and table name which can be used in a case statement to determine which field should be used)
			X how to figure out which field values to use when generating the URLs (e.g. CRUISE_ID vs. CRUISE_LEG_ID but also which CRUISE_LEG_ID should be used when the errors are associated with the parent, not the corresponding child record?)
		DVM error type is used to generate DVM Errors (error records linked to CCD_CRUISES record)
			X **Use the placeholder approach for constructing the necessary pieces of the URL and the rest of it can be determined at runtime like APP_ID and session id

	X (yes) update the LAST_EVAL_DATE for the DVM_PTA_RULE_SETS record each time it is evaluated:


	X foundational views:
		(X NO NEED) DVM_ISSUE_TYPES_V (DVM_ERROR_TYPES, DVM_ERR_SEVERITY)


		(X renamed from DVM_QC_CRITERIA_V) X DVM_CRITERIA_V (DVM_QC_OBJECTS, DVM_ERROR_TYPES_V, DVM_DATA_STREAMS_V)

		X DVM_RULE_SETS_V (DVM_PTA_RULE_SETS, DVM_ISS_TYP_ASSOC, DVM_QC_CRITERIA_V)

		X DVM_PTA_ERRORS_V (DVM_ERRORS, DVM_PTA_ERRORS, DVM_ERR_RES_TYPES, DVM_QC_CRITERIA_V)

	X query for the validation criteria using DBMS_SQL so that we can use multiple data stream codes?

	X semantic change - use issue instead of error since there are also warnings that are not considered errors

	X change the missing placeholder QC query to use a loop instead of bulk collect (still not sure why this doesn't work in 12c - this was resolved by rewriting the query)

	X update the DVM triggers for all tables with auditing fields (DDL Helper)

	X convert all documentation to MD files and link them together (_ spreadsheets must be maintained as .xlsx or other open format to allow them to be used by the public)


	X update the code to ensure that only the placeholders are replaced not every single field (optimization) - right now if there are 100 fields it will attempt to replace all of them even if there are no placeholders
		X do this is a SP that takes the result set and the specified field and replaces all found placeholders instead of all of them

		X change the QC query in the DVM_PKG to ignore the two reserved placeholder values ([APP_SESSION], [APP_ID])

	X - how do we reduce the amount of error type assoc intersection records?  - Use a PTA strategy to define the error types for a given date range:
    X process to release an update - use a procedure to save the error records when they're updated (each time they are updated update the currently active PTA issue type assoc record to set the end date to the active date)


	X - need to handle replacing existing errors so that the error records are not purged each time the module is executed:
	    X - The pending and existing error records are compared to determine if the ERROR_DESCRIPTION and ERROR_TYPE_ID are the same.  All existing error records that do not match a pending record are deleted from the database, all matching records are left untouched, and all unmatched pending records are inserted into the database.

	X - implement transactions

	X - change DVM_ERRORS.ERROR_DESCRIPTION and DVM_ERROR_TYPES.ERR_TYPE_COMMENT_TEMPLATE to a CLOB data type:

	X ** RETRIEVE_ACTIVE_RULE_SET_SP needs to be modified for the DEFINE_RULE_SET_SP (this needs to take an array of data stream codes as an argument) as well

	X DVM PKG: pre check the data stream codes provided are all valid otherwise throw an error

Use cases:
  - (added to Google Keep - 7/9/20) What if the error description template changes, then it will never match up - how is this handled?

	X if the record has been validated before
		X query for the associated issue types based on the associated rule set: (RETRIEVE_QC_CRITERIA)
	X if the record has not been validated before
		X query for the rules for the active rule set (only one should exist)
		X query for the rules that are currently active and compare the lists (two delimited lists with an equals operator)

	X need to test DVM with multiple data streams
		X rule sets must be defined at the data stream level (keep them separate even if they are called together)
			X (process them together) may need to process them separately or there may be some gains from evaluating them together

	X Will the active rule set work for each data stream (e.g. RPL, UL, FOT) -> how do we keep these from interfering with each other?
		X we just query for the rule sets separately as they are specified by the corresponding data streams in the DVM executions

	X TEST CASES: split the CCD data stream into two different data streams and split some criteria in the same view objects for testing purposes
		X this will allow them to be executed independently or together

	X Multiple data streams defined for a single parent table
		X need to be able to handle the use case where one of the data streams was evaluated and then the other one was (there is already a DVM_PTA_ISSUES record but no associated DVM_PTA_RULE_SETS record for the specified data stream)
	X multiple data streams defined for different parent tables
		X allow the data streams to be evaluated separately
	X allow multiple rule sets (based on data stream) to be evaluated separately (do not purge the rule sets

	X **evaluate one data stream, evaluate the other data stream, evaluate the first data stream again (this causes the second data stream to be re-evaluated and the duplicate error records are added from the second data stream)
		X this was resolved by initializing the ALL_CRITERIA member variable in the package definition on each execution (it would retain the values from the last execution unless it was cleared with .delete)

	X - Give parent record does not exist

	X - No DVM_PTA_ERRORS record for the given parent record

	X - A DVM_PTA_ERRORS record exists for the give parent record

	X - A DVM_PTA_ERRORS record and error records exist for the give parent record

	X (HI1001) validate a record for the first time with the active criteria
		X update the validation rules to disable the issue types for some validation issues that it's associated with (Missing Cruise Primary Survey Category, Missing Leg Gear)
			X revalidate record and confirm the LAST_EVAL_DATE is updated and the same validation issue records exist (they are still associated with the active rule set)
		X (RL-17-05) validate a record that has the issues that were disabled and confirm that those are not identified with the new record

	X - Give parent record does not exist

	X - No DVM_PTA_ERRORS record for the given parent record

	X - A DVM_PTA_ERRORS record exists for the give parent record

	X - A DVM_PTA_ERRORS record and error records exist for the give parent record


	X **How do we migrate during the upgrade
		X if the same validation criteria were active during two different periods then the migration of the DVM data must be handled on a case by case basis

		X as long as the validation criteria that were evaluated changed to unique lists of issue types each time we can use the method below - implemented in migrate_error_type_assoc_values.sql)
			X query for the delimited values (order by error_type_id) and then look for unique values with max and min values for the CREATE_DATE

			--query to find the different error groups
			select MIN(CREATE_DATE), MAX(CREATE_DATE), error_type_list,
			LISTAGG(pta_error_id, ', ') WITHIN GROUP (order by pta_error_id) as pta_error_list
			FROM
			(select pta_error_id,
			CREATE_DATE,
			LISTAGG(error_type_id, ', ') WITHIN GROUP (order by error_type_id) as error_type_list

			FROM
			DVM_PTA_ERR_TYP_ASSOC group by pta_error_id, create_date)
			group by error_Type_list
			order by  MIN(CREATE_DATE);

business rules:
	_ policy - should we ignore the inactive rules when evaluating the DVM criteria associated with a given parent error record? No, but we could build in the functionality of refreshing the rules (associate with the currently active rule set instead of the one it was originally validated with)
	_ can't remove columns from queries or else the QC queries will break.
	_ how to handle when existing criteria are modified
	X QC query that only one rule set is active (procedure RETRIEVE_ACTIVE_RULE_SET_SP)
		X (enable when it is created)
		X (disable when it doesn't match the active rules and update the disable date)
	X check if there is more than one active rule set:
	X document the [APP_SESSION] and [APP_ID] APEX placeholder business rules:

design principle:
	X Define the rule sets and reference them instead of associating each issue type with the parent record directly to cut down on the number of records
	X add in the first_eval and last_eval dates for the DVM_PTA_RULE_SETS or a different view so the information is available



--DVM Version 1.0 and greater description:--

Stable validation module
	Logging in the DB for debugging/troubleshooting purposes
	User Defined Exceptions - codes for each type of processing error to facilitate troubleshooting, should be able to pinpoint the location of the issues in the code based on the error codes
	DVM QC configuration queries - to identify problems with the DVM configuration that need to be resolved before it can be executed successfully.  This has been integrated into the SOP after changes are made to the validation rules to ensure the stability of the DVM instance
SOP for implementing the DVM
Automated testing for 7 different categories
	SOP for defining and verifying data set specific test cases
Standard reports for DVM rules, processing history for documentation/auditing purposes
QC configuration queries for developers to understand DVM configuration errors and resolve them before executing the DVM
Examples of data set specific code and application that integrates the DVM.
***Addresses the NOAA data strategy objective of data transparency.
***Formally address the assurance phase of the data life cycle
	Repeatable, documented method of evaluating data QC criteria





DVM Algorithm (as of 6/5/20):

_ Validate Parent Record:
	- Retrieve the data stream info for all data stream codes:
		- data stream exists:
			- retrieve parent record
				- parent record exists
					- retrieve parent error record
						- parent error record exists
							- loop through data stream codes to determine if all of the corresponding rule sets have been evaluated before for the parent error record
								- if one or more rule sets has not been evaluated
									- RETRIEVE_ACTIVE_RULE_SET_SP on the data stream codes that have not been evaluated
										- this will insert the DVM_RULE_SETS record if necessary and the DVM_PTA_RULE_SETS record for the parent error record and data stream code argument(s)
										- the rule_set_id array will be returned so it can be used with RETRIEVE_QC_CRITERIA
										- RETRIEVE_ACTIVE_RULE_SET_SP success:
											- add the rule_set_id values to the v_rule_set_id_array so it contains all rule sets that will be validated

										- RETRIEVE_ACTIVE_RULE_SET_SP failure:
								- if one or more rule sets have been evaluated add the rule_set_id values to the v_rule_set_id_array array
						- parent error record does not exist
							- define the parent error record
								- define parent error record success:
									- RETRIEVE_ACTIVE_RULE_SET_SP on the data stream codes
										- this will insert the DVM_RULE_SETS record if necessary and the DVM_PTA_RULE_SETS record for the parent error record and data stream code argument(s)
										- the rule_set_id array will be returned so it can be used with RETRIEVE_QC_CRITERIA
										- RETRIEVE_ACTIVE_RULE_SET_SP success:
											-associate parent record with parent error record
												- associate parent record success:
													- continue processing
												- associate parent record failure:
										- RETRIEVE_ACTIVE_RULE_SET_SP failure:
								- define parent error record error:
				- parent record does not exist
		- data stream doesn't exist:


		--continue processing:
			- RETRIEVE_QC_CRITERIA for the specified rule_set_id array
				- RETRIEVE_QC_CRITERIA success:
					- EVAL_QC_CRITERIA()
						- EVAL_QC_CRITERIA() success
						- EVAL_QC_CRITERIA() failure
				- RETRIEVE_QC_CRITERIA failure:



/********************************************/
--Content from old DVM technical document:
/********************************************/


## Legend
<mark>Completed</mark>

## Future plans for additional/modified functionality:
- <mark>Develop import process to allow the QC template spreadsheet to be imported directly into the DVM_QC_OBJECTS and DVM_ISS_TYPES tables to remove the requirement to manually enter this data which can be time-consuming and potentially issue-prone (possibly just import into temp table and use merge queries to insert/update records into the corresponding tables as necessary).</mark>
  - This is currently implemented in Excel formulas that generate the necessary DML to load the records
- <mark>Update algorithm to associate the issue records directly with the parent record (e.g. SPT_VESSEL_TRIPS vs. SPT_APP_XML_FILES since only the RPL data entered via XML import module will have an associated SPT_APP_XML_FILES record).  When this change is made there would be no distinction between parent issue records and parent records.
- <mark>Enhance the validation module to maintain existing issue records that are identical to the new issue records and retain any annotations made to these records so as not to lose the manual work completed by data management staff.  This is done by comparing the ISS_DESC and ISS_TYPE_ID values between existing and pending issue records to determine if existing records should be deleted and/or new records should be added
- <mark>Implement the Data Validation Module as a stand-alone PL/SQL package that can be used directly in the database and executed via APEX, PHP, etc.  The logic in the evaluate_QC_criteria() and re_run_validation() methods would need to be implemented in stored procedures in a package.
	- <mark>Query update - query for the primary key field name based on the DATA_STREAM_PAR_TABLE field in the DVM_DATA_STREAMS table so that it can be used as an argument in the data validation package.
	- Include a stored procedure that can batch re-evaluate records
- <mark>Implement URL generating capabilities into the framework to allow the QC Views to generate their own URLs for application pages that will allow the given issue to be resolved easily (e.g. APEX edit trip event link, PHP DM page link).  We would need to add to the fields in the DVM_ISSUES table and define a designated IND_FIELD to pull this information from (e.g. DATA_URL).  Could implement in foundational views as well and just pull directly into QC query as the link could be valuable in other contexts</mark>
- There is a slow query that needs to be fixed when time permits (SPT_QC_EVT_DIST_ISSUE_V).  The execution slows down considerably as the database is populated.
- <mark>Develop simple generalized interface to allow validation issues to be annotated in web interface (prototype completed in PARR App, CRDMA, and CDMA)</mark>
	- X develop this further
	- Could be part of a more generalized application that could be used throughout the Center
- <mark>Look into combining and generalizing the SPT_PTA_ERRORS and SPT_PTA_ERROR_TYPES tables (e.g. SPT_PTA_VALIDATIONS) since they seem to be redundant.  Could use a single key that two different tables reference.   This would simplify the logic and reduce the requirements for implementing framework on other data streams.
- <mark>Generalize objects and use less-specific names for the different database entities.  (e.g. should not refer to things as errors since some are warnings, maybe use the term "data issue" instead).  All object names should be changed accordingly as well.</mark>
